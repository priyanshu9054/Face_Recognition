{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07565661-8dec-41f7-aa02-e917fa51af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting samples complete\n",
      "Model trained successfully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Face Detection and Face Mesh\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_detection = mp_face_detection.FaceDetection()\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = './face/user_2/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize OpenCV video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Convert the image to RGB format\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image for face detection using MediaPipe\n",
    "    results = face_detection.process(image_rgb)\n",
    "\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            # Get the face landmarks\n",
    "            face_landmarks = face_mesh.process(image_rgb)\n",
    "\n",
    "            if face_landmarks.multi_face_landmarks:\n",
    "                for face_lms in face_landmarks.multi_face_landmarks:\n",
    "                    count += 1\n",
    "                    face_roi = image_rgb\n",
    "\n",
    "                    # Save the face image\n",
    "                    file_name_path = os.path.join(output_dir, f'{count}.jpg')\n",
    "                    cv2.imwrite(file_name_path, cv2.cvtColor(face_roi, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "                    # Draw the face landmarks on the image\n",
    "                    mp_drawing = mp.solutions.drawing_utils\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=frame,\n",
    "                        landmark_list=face_lms,\n",
    "                        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1, circle_radius=1)\n",
    "                    )\n",
    "\n",
    "                    # Put count on images and display live count\n",
    "                    cv2.putText(frame, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "                    cv2.imshow('Face Cropper', frame)\n",
    "            else:\n",
    "                print(\"Face Not Found!!\")\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100:  # Press 'Enter' to exit or collect 100 samples\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Collecting samples complete\")\n",
    "\n",
    "# Train the face recognition model using OpenCV\n",
    "data_path = './face/user_2/'\n",
    "face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "for i, file in enumerate(os.listdir(data_path)):\n",
    "    image_path = os.path.join(data_path, file)\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if images is None:\n",
    "        continue\n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "if len(Training_Data) == 0:\n",
    "    print(\"No training data found. Please ensure there are images in the specified directory.\")\n",
    "    exit()\n",
    "\n",
    "face_recognizer.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "print(\"Model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b130051-3364-421f-a75d-2c7770ec055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to ./face_recognition_model.yml\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a .yml file\n",
    "model_file = './face_recognition_model.yml'\n",
    "face_recognizer.save(model_file)\n",
    "print(f\"Trained model saved to {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0a23d3-0447-4ac2-894f-76858d6fc6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to ./face_recognition_model.yml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Train the face recognition model using OpenCV\n",
    "data_path = './face/user_2/'\n",
    "face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "for i, file in enumerate(os.listdir(data_path)):\n",
    "    image_path = os.path.join(data_path, file)\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if images is None:\n",
    "        continue\n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "if len(Training_Data) == 0:\n",
    "    print(\"No training data found. Please ensure there are images in the specified directory.\")\n",
    "    exit()\n",
    "\n",
    "# Train the model\n",
    "face_recognizer.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "\n",
    "# Save the trained model to a .yml file\n",
    "model_file = './face_recognition_model.yml'\n",
    "face_recognizer.save(model_file)\n",
    "print(f\"Trained model saved to {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0318d-e839-423d-97e9-89781723eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "# Create a FaceDetection object\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "\n",
    "def detect_faces(image):\n",
    "    # Convert the image to RGB format\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the image for face detection\n",
    "    results = face_detection.process(image_rgb)\n",
    "    \n",
    "    # Draw rectangles around the detected faces and print confidence\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            x, y, w, h = int(bbox.xmin * image.shape[1]), int(bbox.ymin * image.shape[0]), int(bbox.width * image.shape[1]), int(bbox.height * image.shape[0])\n",
    "            \n",
    "            # Extract the face ROI\n",
    "            face_roi = image[y:y+h, x:x+w]\n",
    "            face_gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Perform face recognition\n",
    "            label, confidence = face_recognizer.predict(face_gray)\n",
    "            \n",
    "            # Define bounding box color based on recognition confidence\n",
    "            if confidence < 50:\n",
    "                color = (0, 0, 255)  # Red color for low confidence\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green color for high confidence\n",
    "            \n",
    "            # Draw the bounding box with the appropriate color\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            \n",
    "            # Print the confidence\n",
    "            cv2.putText(image, f\"Confidence: {confidence}%\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# For webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        result = detect_faces(frame)\n",
    "        cv2.imshow('Face Detection', result)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d5e96-7f35-49f0-b5f9-36f7794919a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceRec",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
